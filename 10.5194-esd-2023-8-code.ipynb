{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a933f4c",
   "metadata": {},
   "source": [
    "# Compute seasonal mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "407b7f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeSeasonalMean(DateArr,idata,season='DJF'):\n",
    "\t\"\"\"\n",
    "\tINPUT: \n",
    "        DateArr [time] (list of datetime objects)\n",
    "        idata [lats,lons,time]\n",
    "        season='DJF'\n",
    "\tOUPTUT: DateArr_season, odata\n",
    "\t\"\"\"\n",
    "\n",
    "\timport pandas as pd\n",
    "\timport numpy as np\n",
    "\timport matplotlib.pylab as plt\n",
    "\n",
    "\tMonths = 'NDJFMAMJJASOND'\n",
    "\tDaysPerMonth = np.array([30,31,31,28,31,30,31,30,31,31,30,31,30,31])\n",
    "\tIndexMonth = np.array([11,12,1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "\t\t\n",
    "\tnmonth = len(season)\n",
    "\tnh = int((nmonth-1)//2)\n",
    "\t\t\n",
    "\tidata = np.ma.array(idata)\n",
    "\t\n",
    "\tif idata.ndim == 1:\n",
    "\t\tidata = idata[np.newaxis,np.newaxis,:]\n",
    "\t\t\n",
    "\ttime_dim = idata.shape[2]\t\t\n",
    "\t\t\n",
    "\tif len(DateArr) != time_dim:\n",
    "\t\tprint('ERROR! Inconsistent lengths.')\n",
    "\t\treturn False\t\n",
    "\t\t\n",
    "\tif nmonth % 2 == 0:\n",
    "\t\tprint('ERROR! Wrong length of season. Season must include an odd number of months.')\n",
    "\t\treturn False\t\n",
    "\n",
    "\ttry:\n",
    "\t\tDPM_season = DaysPerMonth[Months.index(season):Months.index(season)+nmonth] \n",
    "\t\tcentral_month = IndexMonth[Months.index(season)+nh]\n",
    "\texcept:\n",
    "\t\tprint('ERROR! Undefined season.')\n",
    "\t\treturn False\n",
    "\n",
    "\n",
    "\tDateArr_season = []\n",
    "\tfor i in range(time_dim):\n",
    "\t\tif DateArr[i].month == central_month:\n",
    "\t\t\tDateArr_season += [DateArr[i]]\t\t\n",
    "\n",
    "\todata = np.ma.zeros([idata.shape[0],idata.shape[1],len(DateArr_season)])\n",
    "\tcnto = 0\n",
    "\t\n",
    "\tfor i in range(time_dim):\n",
    "\t\tif 'F' in season:\n",
    "\t\t\tif DateArr[i].year%4 == 0:\n",
    "\t\t\t\tDPM_season[season.index('F')] = 29\n",
    "\t\t\telse:\n",
    "\t\t\t\tDPM_season[season.index('F')] = 28\n",
    "\t\t    \n",
    "\t\tif DateArr[i].month == central_month:\t\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\todata[:,:,cnto] = np.sum(idata[:,:,i:i+nh+1]*DPM_season[nh:],axis=2)/np.sum(DPM_season[nh:])\n",
    "\t\t\telif i == time_dim - 1:\n",
    "\t\t\t\todata[:,:,cnto] = np.sum(idata[:,:,i-nh:i+1]*DPM_season[:nh+1],axis=2)/np.sum(DPM_season[:nh+1])\n",
    "\t\t\telse:\n",
    "\t\t\t\todata[:,:,cnto] = np.sum(idata[:,:,i-nh:i+nh+1]*DPM_season[:],axis=2)/np.sum(DPM_season)\n",
    "\t\t\t\t\n",
    "\t\t\tcnto += 1 \n",
    "\n",
    "\todata = np.squeeze(odata)\n",
    "\treturn DateArr_season, odata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d92edb",
   "metadata": {},
   "source": [
    "# Compute significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "942e9672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeSignificanceArray(idata, DateArr, CL=0.95, factor=120, ano=False, ac='ar1'):\n",
    "    # nicht für wechselnde Maske\n",
    "    # Anomalien werden berechnet, dh es können auch volle Daten in idata übergeben werden\n",
    "    \n",
    "    import numpy as np\n",
    "    import scipy.stats as stats\n",
    "    from sklearn import linear_model\n",
    "    \n",
    "    full_shape = idata.shape\n",
    "    mask = idata[:,:,0].mask # maske wird nur von 1. zeitschritt übernommen!!!\n",
    "    index = np.where(mask == False)\n",
    "    \n",
    "    idata = idata[index][:,np.newaxis,:]\n",
    "    shape = idata.shape\n",
    "\n",
    "    if ano == False:\n",
    "        anomalies, _ = ComputeAnomalyClimatology(idata, DateArr)\n",
    "        idata_anom = anomalies.reshape([shape[0]*shape[1],shape[2]]).T\n",
    "    else:\n",
    "        anomalies = idata[:,:,:]\n",
    "        idata_anom = idata.reshape([shape[0]*shape[1],shape[2]]).T\n",
    "         \n",
    "    xdata = np.arange(len(DateArr))  \n",
    "    \n",
    "    Ntrue = len(DateArr)\n",
    "    alpha = 1 - CL\n",
    "    \n",
    "    model_ols =  linear_model.LinearRegression()\n",
    "    model_ols.fit(xdata.reshape(-1, 1),idata_anom) \n",
    "    idata_anom_trend = model_ols.coef_[:,0] \n",
    "\n",
    "    SignificanceArray = np.zeros([shape[0]*shape[1]])\n",
    "    for i in range(len(idata_anom_trend)):\n",
    "        #if np.ma.is_masked(idata_anom[0,i]): continue\n",
    "        y_fit = model_ols.intercept_[i] + xdata[:]*idata_anom_trend[i]\n",
    "        regression_residual = idata_anom[:,i] - y_fit[:]\n",
    "        r = np.corrcoef(regression_residual[1:],regression_residual[:-1])[0,1]\n",
    "        #Neff = Ntrue*(1-r)/(1+r)\n",
    "        Neff = EffectiveSampleSize(regression_residual,ac=ac)\n",
    "        residual_variance = 1/(Neff)*np.sum(regression_residual[:]**2.)\n",
    "        StandardError = np.sqrt(residual_variance/(np.sum((xdata[:] - np.mean(xdata[:]))**2.))) \n",
    "        \n",
    "        df = Neff - 2\n",
    "        tvalue = stats.t.ppf(1. - alpha/2.,df)\n",
    "        beta_0 = 0. # i.e. the null hypothesis is that x and y are uncorrelated -> no trend\n",
    "        tstatistic = np.abs((idata_anom_trend[i] - beta_0)/StandardError)\n",
    "        SignificanceArray[i] = (0 if tstatistic < tvalue else 1)\n",
    "        \n",
    "    trend = idata_anom_trend.reshape([shape[0],shape[1]])*factor\n",
    "    significance = SignificanceArray.reshape([shape[0],shape[1]])\n",
    "       \n",
    "    anomalies_out = np.ma.zeros(full_shape)\n",
    "    anomalies_out.mask = [True]\n",
    "    \n",
    "    trend_out = np.ma.zeros([full_shape[0],full_shape[1]])\n",
    "    trend_out.mask = [True]\n",
    "    \n",
    "    significance_out = np.ma.zeros([full_shape[0],full_shape[1]])\n",
    "    significance_out.mask = [True]   \n",
    "    \n",
    "    anomalies_out[index] = anomalies[:,0,:]\n",
    "    anomalies_out.mask[index] = False\n",
    "    \n",
    "    trend_out[index] = trend[:,0]\n",
    "    trend_out.mask[index] = False\n",
    "    \n",
    "    significance_out[index] = significance[:,0]\n",
    "    significance_out.mask[index] = False \n",
    "       \n",
    "    if ano == False:\n",
    "        return anomalies_out, trend_out, significance_out\n",
    "    else:\n",
    "        return trend_out, significance_out\n",
    "    \n",
    "    \n",
    "def EffectiveSampleSize(sample,ac='ar1',alpha_th=0.1,CL=0.95):\n",
    "\timport numpy as np\n",
    "\timport scipy.stats as stats\n",
    "\n",
    "\tNtrue = len(sample)\n",
    "\tif ac == 'ar1':\n",
    "\t\talpha = np.corrcoef(sample[1:],sample[:-1])[0,1]\n",
    "\t\tNeff = Ntrue*(1-alpha)/(1+alpha)\n",
    "\t\treturn Neff\n",
    "\t\t\n",
    "\telif ac == 'full':\n",
    "\t\talpha = []\n",
    "\t\tfor lag in range(1,Ntrue-1):\n",
    "\t\t\talpha += [np.corrcoef(sample[lag:],sample[:-lag])[0,1]]\n",
    "\t\t\n",
    "\t\talpha = np.array(alpha)\n",
    "\t\t\n",
    "\t\tsigma_k_squared = [1/Ntrue]\n",
    "\t\tfor i in range(1,len(alpha)):\n",
    "\t\t\tsigma_k_squared += [1/Ntrue*(1+2*np.sum(alpha[:i]**2.))]\n",
    "\t\t\t\n",
    "\t\tsigma_k_squared = np.array(sigma_k_squared)\n",
    "\t\t\n",
    "\t\talpha_cl = 1 - CL\n",
    "\t\ttval = stats.t.ppf(1. - alpha_cl/2.,Ntrue-1)\n",
    "\t\t\n",
    "\t\talpha_signif = np.copy(alpha)\n",
    "\t\tsel_m = 0\n",
    "\t\tfor m in range(len(alpha)-2):\n",
    "\t\t\tif alpha[m+1] < 0 and alpha[m+1] + alpha[m+2] < 0:\n",
    "\t\t\t\tsel_m = m + 1\n",
    "\t\t\t\tbreak\n",
    "\t\t\n",
    "\t\tfor m in range(sel_m):\n",
    "\t\t\tif tval*np.sqrt(sigma_k_squared[m]) > alpha[m] > -tval*np.sqrt(sigma_k_squared[m]):\n",
    "\t\t\t\talpha[m] = 0.\n",
    "\t\t\n",
    "\t\tNeff = Ntrue/(1+2*np.sum(alpha[:sel_m]))\n",
    "\n",
    "\t\treturn Neff\n",
    "\telse:\n",
    "\t\tprint('ERROR! Wrong type given!')\n",
    "\t\treturn Ntrue\n",
    "    \n",
    "def ComputeAnomalyClimatology(idata,DateArr,refdate = [None], dfmt='%Y-%m-%d',print_info=False):\n",
    "\t# refdate as vector including start and enddate as str, e.g.: ['2000-01-15','2009-12-15']\n",
    "\t\"\"\"\n",
    "\tInput: idata,DateArr,refdate = [None], dfmt='%Y-%m-%d'\n",
    "\tOutput: anomaly, climatology\n",
    "\t\"\"\"\n",
    "\n",
    "\timport numpy as np\n",
    "\timport datetime\n",
    "\tnlat = idata.shape[0]\n",
    "\tnlon = idata.shape[1]\n",
    "\tntime = idata.shape[2]\n",
    "\tanomaly = np.ma.zeros(idata.shape)\n",
    "\tclimatology = np.ma.zeros([nlat,nlon,12])\n",
    "\tiorder = np.arange(12)\n",
    "\n",
    "\t\n",
    "\tif refdate[0] == None:\n",
    "\t\t# if no reference period is given\n",
    "\n",
    "\t\tif DateArr[0].month != 1:\n",
    "\t\t\tiorder = np.roll(iorder,-(12-DateArr[0].month+1))\n",
    "\n",
    "\t\tfor i in range(12):\n",
    "\t\t\tif print_info: print(f'Compute climatology {i}/12.')\n",
    "\t\t\tclimatology[:,:,i] = np.nanmean(idata[:,:,iorder[i]::12],axis=2)\n",
    "\n",
    "\t\tif print_info: print('')\n",
    "\t\tfor i in range(ntime):\n",
    "\t\t\tif print_info: print(f'Compute anomaly {i}/{ntime}.')\n",
    "\t\t\tanomaly[:,:,i] = idata[:,:,i] - climatology[:,:,DateArr[i].month-1]\n",
    "\t\t\n",
    "\telse:\n",
    "\t\t# computes climatology with respect to a reference period\n",
    "\t\t# anomalies relative to reference climatology, but for the whole period given by 'DateArr'\n",
    "\n",
    "\t\tif all([type(i) == str for i in refdate]):\n",
    "\t\t\tref_sdate = datetime.datetime.strptime(refdate[0],dfmt)\n",
    "\t\t\tref_edate = datetime.datetime.strptime(refdate[1],dfmt)\n",
    "\t\telse:\n",
    "\t\t\tref_sdate = refdata[0]\n",
    "\t\t\tref_edate = refdata[1]\n",
    "\n",
    "\t\tsi = 0\n",
    "\t\tei = -1\n",
    "\t\tfor i in range(ntime):\n",
    "\t\t\tif DateArr[i] == ref_sdate:\n",
    "\t\t\t\tsi = i\n",
    "\t\t\tif DateArr[i] == ref_edate:\n",
    "\t\t\t\tei = i+1\n",
    "\n",
    "\t\tref_data = idata[:,:,si:ei]\n",
    "\t\tref_date = DateArr[si:ei]\n",
    "\t\tprint(f' -- reference date: {ref_date[0]} to {ref_date[-1]}')\n",
    "\t\t\t\t\n",
    "\t\tif ref_date[0].month != 1:\n",
    "\t\t\tiorder = np.roll(iorder,-(12-ref_date[0].month+1))\t\n",
    "\t\tfor i in range(12):\n",
    "\t\t\tclimatology[:,:,i] = np.nanmean(ref_data[:,:,iorder[i]::12],axis=2)\t\n",
    "\t\t\n",
    "\t\tfor i in range(ntime):\n",
    "\t\t\tanomaly[:,:,i] = idata[:,:,i] - climatology[:,:,DateArr[i].month-1]\n",
    "\n",
    "\treturn anomaly, climatology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa17e3e4",
   "metadata": {},
   "source": [
    "# Linearized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5987e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linearized_Model(rhoao, ws10m, deltaQ, deltaT, no_coeff = True, Cx_SLHF=0.001, Cx_SSHF=0.001, print_vals=False):\n",
    "\t\"\"\"\n",
    "\tinput: rhoao, ws10m, deltaQ, deltaT, no_coeff = True, Cx_SLHF=0.001, Cx_SSHF=0.001, print_vals=True\n",
    "\toutput: lin_slhf, lin_sshf, simple_slhf, simple_sshf, Cx_SLHF, Cx_SSHF\n",
    "\t\"\"\" \n",
    "\n",
    "\timport numpy as np\n",
    "    \n",
    "\tg = 9.80665\n",
    "\tcp = 1004.7090 \n",
    "\tLv = 2.5008e6\n",
    "\tzn = 10.\n",
    "\n",
    "\tws_mean = np.mean(ws10m)\n",
    "\tws_prim = ws10m - ws_mean\n",
    "\n",
    "\trho_mean = np.mean(rhoao)\n",
    "\trho_prim = rhoao - rho_mean\n",
    "\n",
    "\tdq_mean = np.mean(deltaQ)\n",
    "\tdq_prim = deltaQ - dq_mean\n",
    "\n",
    "\tt_mean = np.mean(deltaT)\n",
    "\tt_prim = deltaT - t_mean\n",
    "\n",
    "\tif print_vals:\n",
    "\t\tprint(f'Wind mean, std: ({ws_mean:.2e}, {np.std(ws_prim):.2e})')  \n",
    "\t\tprint(f'   Q mean, std: ({dq_mean:.2e}, {np.std(dq_prim):.2e})') \n",
    "\t\tprint(f'   T mean, std: ({t_mean:.2e}, {np.std(t_prim):.2e})') \n",
    "    \n",
    "    # SLHF\n",
    "\tlinear_part =    ws_mean*rho_mean*dq_mean + ws_mean*rho_mean*dq_prim + ws_mean*rho_prim*dq_mean + ws_prim*rho_mean*dq_mean\n",
    "\tnonlinear_part = ws_mean*rho_prim*dq_prim + ws_prim*rho_mean*dq_prim + ws_prim*rho_prim*dq_mean + ws_prim*rho_prim*dq_prim\n",
    "\n",
    "\tif no_coeff:\n",
    "\t\tlin_slhf =    Lv*(linear_part)\n",
    "\t\tnonlin_slhf = Lv*(linear_part + nonlinear_part)\t\n",
    "\telse:\n",
    "\t\tlin_slhf =    Lv*Cx_SLHF*(linear_part)\n",
    "\t\tnonlin_slhf = Lv*Cx_SLHF*(linear_part + nonlinear_part)\n",
    "\n",
    "    # SSHF\n",
    "\tlinear_part_sh =    ws_mean*rho_mean*t_mean + ws_mean*rho_mean*t_prim + ws_mean*rho_prim*t_mean + ws_prim*rho_mean*t_mean\n",
    "\tnonlinear_part_sh = ws_mean*rho_prim*t_prim + ws_prim*rho_mean*t_prim + ws_prim*rho_prim*t_mean + ws_prim*rho_prim*t_prim\n",
    "\n",
    "\tlin_gz_part = g*zn*(ws_mean*rho_mean + ws_mean*rho_prim  + ws_prim*rho_mean)\n",
    "\tnonlin_gz_part = g*zn*(ws_prim*rho_prim)\n",
    "\n",
    "\tif no_coeff:\n",
    "\t\tlin_sshf =    cp*(linear_part_sh) + lin_gz_part\n",
    "\t\tnonlin_sshf = cp*(linear_part_sh + nonlinear_part_sh) + nonlin_gz_part\n",
    "\telse:\n",
    "\t\tlin_sshf =    cp*Cx_SSHF*(linear_part_sh) +  Cx_SSHF*lin_gz_part\n",
    "\t\tnonlin_sshf = cp*Cx_SSHF*(linear_part_sh + nonlinear_part_sh) + Cx_SSHF*nonlin_gz_part\n",
    "    \n",
    "\treturn lin_slhf, lin_sshf, nonlin_slhf, nonlin_sshf, Cx_SLHF, Cx_SSHF\n",
    "\n",
    "\n",
    "\n",
    "def Compute_TransferCoefficient(RefFlux,NonLinModel_woCx,LinModel_woCx,multiplier=1,print_info=True):\n",
    "\t\"\"\"\n",
    "\tinput: RefFlux,NonLinModel_woCx,LinModel_woCx\n",
    "\toutput: NonLinModel_adjusted, LinModel_adjusted\n",
    "\t\"\"\"\n",
    "\timport numpy as np\n",
    "\tfrom sklearn import linear_model\n",
    "\n",
    "\t# Compute reference trend (i.e. the target trend)\n",
    "\tmodel_ols =  linear_model.LinearRegression() \n",
    "\ttime = np.arange(len(RefFlux)).reshape(-1,1)\n",
    "\tmodel_ols.fit(time,RefFlux)\n",
    "\treference_trend = model_ols.coef_[0]\n",
    "\n",
    "\t# Compute trend of non-linear model\n",
    "\ttime = np.arange(len(NonLinModel_woCx)).reshape(-1,1)\n",
    "\tmodel_ols.fit(time,NonLinModel_woCx)\n",
    "\tmodel_trend = model_ols.coef_[0]\t\n",
    "\n",
    "\t# Compute trend of linear model\n",
    "\ttime = np.arange(len(LinModel_woCx)).reshape(-1,1)\n",
    "\tmodel_ols.fit(time,LinModel_woCx)\n",
    "\tlin_model_trend = model_ols.coef_[0]\t\n",
    "\n",
    "\tCx = reference_trend/model_trend\n",
    "\tNonLinModel_adjusted = Cx * NonLinModel_woCx\n",
    "\tLinModel_adjusted = Cx * LinModel_woCx\n",
    "\n",
    "\tif print_info:\n",
    "\t\tprint(f' Computed transfer coefficient: {Cx:.8f}')\n",
    "\t\tprint(f'  Reference trend: {reference_trend*multiplier:.5f}')\n",
    "\t\tprint(f'lin+non-lin model: {model_trend*Cx*multiplier:.5f}')\n",
    "\t\tprint(f'     linear model: {lin_model_trend*Cx*multiplier:.5f}')\n",
    "\n",
    "\treturn NonLinModel_adjusted, LinModel_adjusted, Cx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0420af",
   "metadata": {},
   "source": [
    "# Compute trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4be3e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trend(DateArr,idata,factor=120,print_trend=True):\n",
    "\t\"\"\"\n",
    "\tInput: DateArr,idata,factor=120,print_trend=True\n",
    "\tOutput: model_ols.coef_[0]*factor, model_ols.intercept_\n",
    "\t\"\"\"\n",
    "    \n",
    "\tfrom sklearn import linear_model\n",
    "\timport numpy as np\n",
    "\t\n",
    "\tndims = idata.ndim\n",
    "\tshape = idata.shape\n",
    "\tmodel_ols =  linear_model.LinearRegression() \n",
    "\ttime = np.arange(len(DateArr)).reshape(-1,1)\n",
    "\n",
    "\tif ndims == 3 and shape[2] == len(DateArr):\n",
    "\t\t#idata = idata.reshape([shape[0]*shape[1],len(DateArr)]).T\n",
    "\t\t\n",
    "\t\tmask = idata.mask[:,:,0]\t\t# wähle maske basierend auf 1. zeitschritt aus\n",
    "\t\tindex = np.where(mask == False)\t# extrahiere alle indizes, welche nicht maskiert sind,\n",
    "\t\tidata = idata[index].T\t\t\t# reduziere daten auf ausgewählte indizes. letzte dimension (zeit) bleibt erhalten\n",
    "\t\t\n",
    "\telif ndims == 2 and shape[1] == len(DateArr):\n",
    "\t\tidata = idata.T\n",
    "\n",
    "\tmodel_ols.fit(time, idata) \n",
    "\n",
    "\tif ndims == 3:\n",
    "\t\ttrend_out = np.ma.zeros([shape[0],shape[1]])\t# erstelle globales gitter, aber mit maske\n",
    "\t\ttrend_out.mask = [True]\t\t\t\t\t\t\t# notwendig weil np.ma.zeros kein 'mask=' argument hat\n",
    "\t\ttrend_out.mask[index] = False\t\t\t\t\t# entferne maske von allen gitterpunkten, die verwendet werden\n",
    "\t\ttrend_out[index] = model_ols.coef_[:,0]*factor\t# füge daten in output array ein\n",
    "    \t\n",
    "\t\tinterc_out = np.ma.zeros([shape[0],shape[1]])\n",
    "\t\tinterc_out.mask = [True]\n",
    "\t\tinterc_out.mask[index] = False\n",
    "\t\tinterc_out[index] = model_ols.intercept_*factor    \t\n",
    "\t\n",
    "\t\treturn trend_out, interc_out\n",
    "\t\t#return model_ols.coef_.reshape([shape[0],shape[1]])*factor, model_ols.intercept_.reshape([shape[0],shape[1]])\n",
    "\telif ndims == 2: \n",
    "\t\treturn model_ols.coef_.reshape([shape[0]])*factor, model_ols.intercept_.reshape([shape[0]])\n",
    "\telif ndims == 1: \n",
    "\t\tif print_trend: print(f'{model_ols.coef_[0]*factor:.4f}')\n",
    "\t\treturn model_ols.coef_[0]*factor, model_ols.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26561461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TS_ApplyMovingAverage(idata=[],window=12):\n",
    "\t\"\"\"\n",
    "\tinput:  idata,window=12 (i.e., total window size -> +-window/2)\n",
    "\toutput: odata (smoothed timeseries)\n",
    "\t\"\"\"\n",
    "\n",
    "\tfrom sys import path\n",
    "\timport numpy as np\n",
    "\n",
    "\tif len(idata) == 0:\n",
    "\t\tprint(' Input: idata')\n",
    "\t\tprint('Output: smoothed idata')\n",
    "\t\treturn\n",
    "\n",
    "\trmw = np.ones(window)\n",
    "\tndata = len(idata)\n",
    "\todata = np.ma.zeros(ndata)\n",
    "\n",
    "\tfor i in range(ndata):\t\t\t\t\t# loop over all DATA \n",
    "\t\tswitch0 = 1\t\t\t\t\t# switch: \n",
    "\t\tfor j in range(len(rmw)):\t\t\t# loop over all WEIGHTS\n",
    "\t\t\trmi = i - (len(rmw)-1)//2 + j\t\t# running mean index: data index (i) minus half number of weights plus weight index (j)\n",
    "\t\t\t\t\t\t\t\t\t\t\t# asymmetrisch: dh bei len(rmw)==12 werden die 5 Monate vor und 6 Monate nach Zielmonate verw.\n",
    "\t\t\tif rmi < 0: continue\t\t\t# if index is smaller than 0, continue to next weight. important for left boundary\n",
    "\t\t\tif rmi > ndata-1: continue\t\t# important for right boundary\n",
    "\t\t\todata[i] = odata[i] + idata[rmi]*rmw[j]\n",
    "\t\t\tindex1 = j\t\t\t\t\t\t\t# index of the last weight that is used\n",
    "\t\t\tif switch0 == 1:\t\t\t\t\t\n",
    "\t\t\t\tindex0 = j\t\t\t\t\t\t# index of the first weight\n",
    "\t\t\t\tswitch0 = 0\t\t\t\t\t\t# turn off switch\n",
    "\t\todata[i] = odata[i]/np.sum(rmw[index0:index1+1])\t# divide by sum of the weights\n",
    "\n",
    "\tMC = np.ma.array([0],mask=[True])[0]\n",
    "\todata[:window//2] = MC #np.NaN\n",
    "\todata[-window//2:] = MC #np.NaN\n",
    "\t#print(odata)\n",
    "\treturn odata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bda7f3",
   "metadata": {},
   "source": [
    "# Compute OHT trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0eea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDateArray(sdate,edate,resol='monthly', dfmt='%Y-%m-%d'):\n",
    "\t# \n",
    "\t\"\"\"\n",
    "\t INPUT: sdate,edate,resol='monthly', dfmt='%Y-%m-%d'\n",
    "\t \t\t(possible resolutions: 'hourly', 'daily', 'monthly', 'yearly')\n",
    "\tOUTPUT: dateArr\n",
    "\t\"\"\"\n",
    "\timport datetime\n",
    "\tfrom inspect import currentframe, getframeinfo\n",
    "\n",
    "\tif type(sdate) == str:\n",
    "\t\tsdate = datetime.datetime.strptime(sdate,dfmt)\n",
    "\tif type(edate) == str:\n",
    "\t\tedate = datetime.datetime.strptime(edate,dfmt)\t\n",
    "\n",
    "\tsyear = sdate.year\n",
    "\tsmonth = sdate.month\n",
    "\tsday = sdate.day\n",
    "\teyear = edate.year\n",
    "\temonth = edate.month\n",
    "\teday = edate.day\n",
    "\n",
    "\tdateArr = []\n",
    "\tif resol == 'hourly':\n",
    "\t\tdt = datetime.timedelta(hours=1)\n",
    "\t\tdate = sdate\n",
    "\t\twhile date <= edate:\n",
    "\t\t\tdateArr += [date]\n",
    "\t\t\tdate = date + dt\n",
    "\n",
    "\telif resol == 'daily':\n",
    "\t\tfor year in range(syear,eyear+1):\n",
    "\t\t\tDaysPerMonths = [31,28,31,30,31,30,31,31,30,31,30,31]\n",
    "\t\t\tif year%4 == 0: DaysPerMonths[1] = 29\n",
    "\n",
    "\t\t\tsmonth_iter = 1\n",
    "\t\t\temonth_iter = 13\n",
    "\t\t\tif year == syear:\n",
    "\t\t\t\tsmonth_iter = smonth\n",
    "\t\t\tif year == eyear:\n",
    "\t\t\t\temonth_iter = emonth + 1\n",
    "\n",
    "\t\t\tfor month in range(smonth_iter,emonth_iter):\n",
    "\t\t\t\tsday_iter = 1\n",
    "\t\t\t\teday_iter = DaysPerMonths[month-1] + 1\n",
    "\t\t\t\tif year == syear and month == smonth:\n",
    "\t\t\t\t\tsday_iter = sday\n",
    "\t\t\t\tif year == eyear and month == emonth:\n",
    "\t\t\t\t\teday_iter = eday + 1\n",
    "\n",
    "\t\t\t\tfor day in range(sday_iter,eday_iter):\n",
    "\t\t\t\t\tdateArr += [datetime.datetime(year,month,day)]\n",
    "\n",
    "\telif resol == 'monthly':\n",
    "\t\tfor year in range(syear,eyear+1):\n",
    "\t\t\tsmonth_iter = 1\n",
    "\t\t\temonth_iter = 13\n",
    "\t\t\tif year == syear:\n",
    "\t\t\t\tsmonth_iter = smonth\n",
    "\t\t\tif year == eyear: \n",
    "\t\t\t\temonth_iter = emonth + 1\n",
    "\t\t\tfor month in range(smonth_iter,emonth_iter):\n",
    "\t\t\t\tdateArr += [datetime.datetime(year,month,sday)]\t\n",
    "\telif resol == 'yearly':\n",
    "\t\tfor year in range(syear,eyear+1):\n",
    "\t\t\tdateArr += [datetime.datetime(year,smonth,sday)]\n",
    "\telse:\n",
    "\t\tframeinfo = getframeinfo(currentframe())\n",
    "\t\tprint(f'ERROR! Wrong resolution selected. Use one of the following: hourly/daily/monthly/yearly ({_currentroutine}:line {frameinfo.lineno+2}).')\n",
    "\t\treturn\n",
    "\n",
    "\treturn dateArr\n",
    "\n",
    "\n",
    "def TS_Compute_Anomaly(idata,date,x_reftime = [None]):\n",
    "\timport numpy as np\n",
    "\timport datetime\n",
    "\tndata = len(idata)\n",
    "\todata_anom = np.ma.zeros(ndata)\n",
    "\todata_clim = np.ma.zeros(12)\n",
    "\tiorder = np.arange(12)\n",
    "\n",
    "\tif x_reftime[0] == None:\n",
    "\t\tif date[0].month != 1:\n",
    "\t\t\tiorder = np.roll(iorder,-(12-date[0].month+1))\n",
    "\n",
    "\t\tfor i in range(12):\n",
    "\t\t\todata_anom[i::12] = idata[i::12] -  np.nanmean(idata[i::12])\n",
    "\t\t\todata_clim[i] = np.nanmean(idata[iorder[i]::12])\n",
    "\telse:\n",
    "\n",
    "\t\tref_clim = np.ma.zeros(12)\n",
    "\t\tref_sdate = datetime.datetime(x_reftime[0],x_reftime[1],date[0].day)\n",
    "\t\tref_edate = datetime.datetime(x_reftime[2],x_reftime[3],date[0].day)\n",
    "\n",
    "\t\tsi = 0\n",
    "\t\tei = -1\n",
    "\t\tfor i in range(len(idata)):\n",
    "\t\t\tif date[i] == ref_sdate:\n",
    "\t\t\t\tsi = i\n",
    "\t\t\tif date[i] == ref_edate:\n",
    "\t\t\t\tei = i+1\n",
    "\n",
    "\t\tref_data = idata[si:ei]\n",
    "\t\tref_date = date[si:ei]\n",
    "\n",
    "\n",
    "\t\tif ref_date[0].month != 1:\n",
    "\t\t\tiorder = np.roll(iorder,-(12-ref_date[0].month+1))\t\n",
    "\t\tfor i in range(12):\n",
    "\t\t\tref_clim[i] = np.nanmean(ref_data[iorder[i]::12])\t\n",
    "\t\t\n",
    "\t\tfor i in range(len(idata)):\n",
    "\t\t\todata_anom[i] = idata[i] - ref_clim[date[i].month-1]\n",
    "\n",
    "\t\todata_clim = np.copy(ref_clim)\n",
    "\t\n",
    "\treturn odata_anom, odata_clim\n",
    "\n",
    "\n",
    "def TS_Compute_CI(idata, DateArr, CL = 0.95, ac='ar1', return_vals = False, print_vals = True, return_01 = False):\n",
    "    \"\"\"\n",
    "    - idata as original time series\n",
    "    - computes confidence intervals for the trend based on anomalies using a two-tailed t-test\n",
    "    - effective sample size (auto-correlation) is considered for computing degrees of freedom and residual variance\n",
    "    - t-test checks whether the trend deviates from 0 (no trend) or not\n",
    "    - default confidence level (CL) is 95 %\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import scipy.stats as stats\n",
    "    from sklearn import linear_model\n",
    "    \n",
    "    data_anom = np.copy(idata)\n",
    "\n",
    "    xdata = np.arange(len(DateArr))  \n",
    "    \n",
    "    model_ols =  linear_model.LinearRegression()\n",
    "    model_ols.fit(xdata.reshape(-1, 1),idata_anom) \n",
    "    idata_anom_trend = model_ols.coef_[0]   \n",
    "\n",
    "    y_fit = model_ols.intercept_ + xdata*idata_anom_trend\n",
    "    \n",
    "    ### regression residual\n",
    "    regression_residual = idata_anom - y_fit\n",
    "    \n",
    "    ### autocorrelation of regr.res.\n",
    "    r = np.corrcoef(regression_residual[1:],regression_residual[:-1])[0,1]\n",
    "    Ntrue = len(idata)\n",
    "    Neff = EffectiveSampleSize(regression_residual,ac=ac)\n",
    "\n",
    "    ### Standard error\n",
    "    residual_variance = 1/(Neff)*np.sum(regression_residual**2.)\n",
    "    StandardError = np.sqrt(residual_variance/(np.sum((xdata - np.mean(xdata))**2.)))\n",
    "#    if print_vals: print(f' -- Standard error = {StandardError:.2f}')\n",
    "    \n",
    "    alpha = 1 - CL\n",
    "    df = Neff - 2\n",
    "    tvalue = stats.t.ppf(1. - alpha/2.,df)\n",
    "    \n",
    "    beta_0 = 0. # i.e. the null hypothesis is that x and y are uncorrelated -> no trend\n",
    "    tstatistic = np.abs((idata_anom_trend - beta_0)/StandardError)\n",
    "    trend_signif = ('NOT ' if tstatistic < tvalue else '')\n",
    "    \n",
    "    ### ALTERNATIVE COMPUTATION\n",
    "    r_time = np.corrcoef(xdata,idata_anom)[0,1]\n",
    "    tstatistic_estimate = np.abs(r_time*np.sqrt(Neff)/np.sqrt(1. - r_time**2))\n",
    "    CI_lower_estimate = stats.norm.ppf(alpha/2., loc=idata_anom_trend, scale=StandardError)\n",
    "    CI_upper_estimate = stats.norm.ppf(1. - alpha/2., loc=idata_anom_trend, scale=StandardError)\n",
    "\n",
    "    CI_upper = idata_anom_trend + tvalue*StandardError\n",
    "    CI_lower = idata_anom_trend - tvalue*StandardError\n",
    "    \n",
    "    if print_vals: print(f' -- (t value,t stat., t stat. est.) = ({tvalue:.1f},{tstatistic:.1f},{tstatistic_estimate:.1f})')\n",
    "    if print_vals: print(f' --    Confidence intervals = ({CI_upper:.3f},{CI_lower:.3f})')\n",
    "    if print_vals: print(f' -- Alternative estimate CI = ({CI_upper_estimate:.3f},{CI_lower_estimate:.3f})\\n\\n')        \n",
    "     \n",
    "    if print_vals: print(f'# Trend is {trend_signif}significant at CL = {CL*100:.1f} %.')        \n",
    "    \n",
    "    if return_vals and return_01:\n",
    "    \treturn np.array([CI_upper,idata_anom_trend,CI_lower]), (0 if tstatistic < tvalue else 1)\n",
    "    elif return_vals:\n",
    "    \treturn np.array([CI_upper,idata_anom_trend,CI_lower])\n",
    "    elif return_01: \n",
    "    \treturn (0 if tstatistic < tvalue else 1)\n",
    "    else:\n",
    "    \treturn\n",
    "\n",
    "\n",
    "\n",
    "def ComputeOHTTrend(mbFS_domain,ohct_domain,DFBSO_pw_extended,corr_global_mean,\\\n",
    "                           sel_anomalies = True, nyrs = 5, csdate = '1950-01-15', cedate = '2019-12-15', no_ohct=False):\n",
    "    # nyrs : number of avering years, for 5-year mean\n",
    "    \n",
    "    DateArr_cut = CreateDateArray('1950-01-15','2019-12-15')   \n",
    "    breite_trend_oht = []\n",
    "    breite_mean_absOHT = []\n",
    "    breite_trend_mbfs = []\n",
    "    breite_trend_ohct = []\n",
    "    breite_area = []\n",
    "    breite_diff_2decades = []\n",
    "    breite_trend_5yr_mean = []\n",
    "\n",
    "    for i in range(90-61,95,5):\n",
    "        print(f'Latitude: {i}')\n",
    "        # MASKING, MOVING AVERAGE, AREA MEAN \n",
    "        \n",
    "        if no_ohct:\n",
    "            diff_domain = mr.ImprovedMasking(mbFS_domain[...],lats,lons,amask=f'{90-i}N 90N 90W 30E')  \n",
    "        else:\n",
    "            diff_domain = mr.ImprovedMasking(mbFS_domain[...] - ohct_domain[...],lats,lons,amask=f'{90-i}N 90N 90W 30E')\n",
    "        \n",
    "        diff_domain_mean = mr.ComputeStats3D(diff_domain,lats,lons)[0]\n",
    "\n",
    "        area = mr.ComputeStats(diff_domain[...,0],lats,lons)[4]\n",
    "        breite_area += [[90-i,area]]\n",
    "\n",
    "        indirect_transport = DFBSO_pw_extended - (diff_domain_mean - corr_global_mean)*area/1e15\n",
    "\n",
    "        if sel_anomalies:\n",
    "            indirect_transport, _ = TS_Compute_Anomaly(indirect_transport,DateArr_cut)\n",
    "\n",
    "                \n",
    "        # MEAN ABSOLUTE TRANSPORT\n",
    "        mean_absOHT_upper, mean_absOHT_lower = CI_mean(indirect_transport,ac='full') #CI_diff_means(indirect_transport[-20:],indirect_transport[:20],ac='full')\n",
    "        breite_mean_absOHT += [[90-i,mean_absOHT_upper,np.ma.mean(indirect_transport),mean_absOHT_lower]]   \n",
    "        \n",
    "        last_decade = np.ma.mean(indirect_transport[-240:])\n",
    "        first_decade = np.ma.mean(indirect_transport[:240])\n",
    "        dec_upper, dec_lower = CI_diff_means(indirect_transport[-240:],indirect_transport[:240],ac='full')\n",
    "\n",
    "        breite_diff_2decades += [[90-i, dec_upper, last_decade-first_decade, dec_lower]]\n",
    "\n",
    "        # TREND OVER 14 5-YEARS MEAN \n",
    "        tyrs = int(cedate[0:4]) - int(csdate[0:4]) + 1 # total years\n",
    "        oht_5yr_mean = np.array([np.ma.mean(indirect_transport[i*nyrs*12:(i+1)*nyrs*12]) for i in range(int(tyrs/nyrs))])\n",
    "        DateArr_5yr = CreateDateArray(csdate,cedate,resol='yearly')[::5]\n",
    "        upper_5yr, trend_5yr, lower_5yr = TS_Compute_CI(oht_5yr_mean,DateArr_5yr,return_vals=True,ac='full',print_vals=False)*2\n",
    "        breite_trend_5yr_mean += [[90-i,upper_5yr, trend_5yr, lower_5yr]]\n",
    "\n",
    "        # MONTHLY TREND \n",
    "        upper, trend, lower = TS_Compute_CI(indirect_transport,DateArr_cut,return_vals=True,ac='full',print_vals=False)*120\n",
    "\n",
    "        breite_trend_oht += [[90-i,upper, trend, lower]]\n",
    "\n",
    "    breite_area = np.array(breite_area)\n",
    "    breite_trend_oht = np.array(breite_trend_oht)\n",
    "    breite_mean_absOHT = np.array(breite_mean_absOHT)\n",
    "    breite_diff_2decades = np.array(breite_diff_2decades)\n",
    "    breite_trend_5yr_mean = np.array(breite_trend_5yr_mean)\n",
    "    \n",
    "    return breite_area, breite_trend_oht, breite_mean_absOHT, breite_diff_2decades, breite_trend_5yr_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a02975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "area, oht_trend, _, _, oht_trend_5yr = ComputeMeridionalTrend(FS,OHCT,OHT_ArcticGateways,R_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3768f2fd",
   "metadata": {},
   "source": [
    "# Compute mass stream function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "148f55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindDateIndex(DateArr,date,dformat='%Y-%m-%d'):\n",
    "    import numpy as np\n",
    "    import datetime\n",
    "    \n",
    "    if isinstance(date,str):\n",
    "        date = datetime.datetime.strptime(date,dformat)\n",
    "    \n",
    "    idx = np.where(np.array(DateArr) == date)[0][0]\n",
    "    \n",
    "    return idx\n",
    "\n",
    "def ComputeStreamFunction(wind_component,levels,spatial_coords,target_axis=0,level_axis=2,time_axis=3):#,order=('lats','lons','levs','time')):\n",
    "    import numpy as np\n",
    "    \n",
    "    REARTH = 6371e3\n",
    "    GCONST = 9.80665\n",
    "    \n",
    "    resol = np.abs(spatial_coords[0]-spatial_coords[1])*(np.pi/180.)\n",
    "\n",
    "    axis_no = np.arange(4)\n",
    "    remainder = axis_no[np.isin(axis_no,[target_axis,level_axis,time_axis],invert=True)][0]\n",
    "    permutation = (target_axis,remainder,level_axis,time_axis)\n",
    "    \n",
    "    #print(permutation)\n",
    "            \n",
    "    wind_component = np.transpose(wind_component,axes=permutation)\n",
    "    \n",
    "    shape = wind_component.shape\n",
    "    #print(shape) #(lats,lons,level,time)\n",
    "    \n",
    "    if shape[2] != len(levels): \n",
    "        print(f'Invalid number of levels.')\n",
    "        return False\n",
    "    \n",
    "    # temporal mean\n",
    "    wind_tmn = np.ma.mean(wind_component,axis=3)\n",
    "    \n",
    "    # zonal/meridional mean\n",
    "    wind_smn = np.ma.mean(wind_component,axis=(3,0))\n",
    "    \n",
    "    wind_ano = np.zeros_like(wind_tmn)\n",
    "    wind_cui = np.zeros_like(wind_tmn)\n",
    "    stream_f = np.ma.zeros([shape[0],shape[2]])\n",
    "      \n",
    "    for i in range(shape[0]):\n",
    "        wind_ano[i,...] = wind_tmn[i,...] - wind_smn[...]\n",
    "        \n",
    "    wind_cui[...,0] = wind_ano[...,0]*levels[0]*100.\n",
    "    \n",
    "    for i in range(1,shape[2]):\n",
    "        wind_cui[...,i] = wind_cui[...,i-1] + wind_ano[...,i]*(levels[i]-levels[i-1])*100.\n",
    "    \n",
    "    for j in range(shape[2]): # über levels (25)\n",
    "        for i in range(shape[0]): # über lons (1440)\n",
    "            stream_f[i,j] = np.ma.sum(wind_cui[i,:,j]*resol)\n",
    "            \n",
    "    return stream_f[...]*(REARTH/GCONST), wind_tmn, wind_smn, wind_ano, wind_cui\n",
    "\n",
    "\n",
    "def PlotStreamFunction(sf,coords,level,nlines=10,figsize=(12,6),fnts = 18,limits=None,savefig=None,fmt='%.1f',plttitle=''):\n",
    "    import numpy as np\n",
    "    import matplotlib.pylab as plt\n",
    "    from matplotlib import ticker\n",
    "    \n",
    "    if limits == None:\n",
    "        lvl = np.arange(sf.min(),sf.max(),(sf.max()-sf.min())/nlines)\n",
    "    else:\n",
    "        lvl = np.arange(limits[0],limits[1],(limits[1]-limits[0])/nlines)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = plt.subplot(1,1,1)\n",
    "    CSp = plt.contour(coords,level,sf,levels=lvl[lvl>0],linewidths=1.0,linestyles='solid',colors='k')\n",
    "    CS0 = plt.contour(coords,level,sf,levels=0,linewidths=3.0,linestyles='solid',colors='k')\n",
    "    CSn = plt.contour(coords,level,sf,levels=lvl[lvl<0],linewidths=1.0,linestyles='dotted',colors='k')\n",
    "    \n",
    "    plt.clabel(CSp,lvl[lvl>0][1::2],inline=1,fmt=fmt,fontsize=10, use_clabeltext=True)\n",
    "    plt.clabel(CS0,[0.],inline=1,fmt=fmt,fontsize=10, use_clabeltext=True)\n",
    "    plt.clabel(CSn,lvl[lvl<0][1::2],inline=1,fmt=fmt,fontsize=10, use_clabeltext=True)\n",
    "    \n",
    "\n",
    "    plt.title(plttitle,loc='right',pad=10)\n",
    "\n",
    "    plt.tick_params(axis='both',which='both',direction='out')\n",
    "                   \n",
    "    plt.xlim(np.min(coords),np.max(coords))\n",
    "    #plt.ylim(np.max(level),np.min(level))\n",
    "    plt.ylim(1000,0)\n",
    "    print(np.min(coords),np.max(coords))\n",
    "    plt.ylabel('hPa',fontsize=fnts)\n",
    "    plt.xticks(fontsize=fnts)\n",
    "    \n",
    "    \n",
    "    if np.array(coords).max() - np.array(coords).min() == 180:\n",
    "        plt.xticks([90,60,30,0,-30,-60,-90],['90°N','60°N','30°N','0°','30°S','60°S','90°S'],fontsize=fnts)\n",
    "        ax.xaxis.set_minor_locator(ticker.FixedLocator([80,70,50,40,20,10,-10,-20,-40,-50,-70,-80]))\n",
    "    else:\n",
    "        plt.xticks([0,90,180,270,360],['0°','90°E','180°','90°W','0°'],fontsize=fnts)\n",
    "        \n",
    "        \n",
    "    plt.yticks(fontsize=fnts)\n",
    "    \n",
    "    ax.yaxis.set_minor_locator(ticker.AutoMinorLocator(2))\n",
    "    \n",
    "    if savefig != None:\n",
    "        plt.savefig(savefig,dpi=300,bbox_inches='tight',facecolor='white')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c3f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "\n",
    "ifile = '1950-2019-V3D-LL320.nc'\n",
    "idata_v = Dataset(ifile, mode='r')\n",
    "\n",
    "lats =  idata_v.variables['latitude'][:]\n",
    "lons =  idata_v.variables['longitude'][:]\n",
    "time =  idata_v.variables['time'][:]\n",
    "level = idata_v.variables['level'][:]\n",
    "\n",
    "time_obj = Convert_Hours_Since_Time(time,return_value=True)\n",
    "\n",
    "sindex = FindDateIndex(time_obj,'1984-12-01')\n",
    "eindex = FindDateIndex(time_obj,'2019-02-01')\n",
    "nseason = 35\n",
    "\n",
    "\n",
    "v_dask = da.from_array(idata_v.variables['v'], chunks=shape)\n",
    "\n",
    "sf_seasonal = np.zeros([nseason,721,25])\n",
    "for i in range(nseason):\n",
    "    print(i)\n",
    "    v_seasonal = np.mean(v_dask[sindex+i*3:sindex+(i+1)*3,...],axis=0)\n",
    "    v_seasonal = v_seasonal.compute()\n",
    "    sf, *_ = ComputeStreamFunction(v_seasonal[np.newaxis,...],level,lons,target_axis=2,level_axis=1,time_axis=0)\n",
    "    sf_seasonal[i,...] = sf[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69048f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
